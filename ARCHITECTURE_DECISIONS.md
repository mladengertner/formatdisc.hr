# ğŸ›ï¸ FormatDisc.hr - Architecture Decision Records (ADRs)

**Status**: Fortune 500 Enterprise Architecture Decisions  
**Date**: December 11, 2025  
**Version**: 1.0  
**Audience**: Enterprise Architects, Technical Leaders, Product Team  

---

## ğŸ“Œ ADR Template

Each ADR follows this structure:
- **Title**: What decision was made?
- **Status**: Accepted | Proposed | Superseded
- **Context**: Why did we need to decide?
- **Decision**: What did we choose and why?
- **Consequences**: What are the trade-offs?
- **Alternatives Considered**: What else could we have done?
- **Related ADRs**: Dependencies on other decisions

---

## ğŸ¯ ADR-001: Immutable Audit Logging with Cryptographic Signatures

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: CTO, Compliance Officer  
**Impact**: CRITICAL (affects all compliance/regulatory work)

### Context

Enterprise clients (especially financial services, healthcare) need **proof** that:
1. Something happened (transaction, deployment, etc.)
2. When it happened (exact timestamp)
3. By whom it happened (actor identity)
4. That nobody changed the record after the fact

Without immutable audit logs, we cannot satisfy GDPR/SOC2/HIPAA requirements, and we expose ourselves to legal liability if records are questioned in a breach investigation.

### Decision

Implement **event sourcing** architecture with cryptographic signatures:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APPLICATION LAYER (e.g., SlavkoKernel.js)              â”‚
â”‚  Emits event: "PROJECT_DEPLOYED"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUDIT EVENT PROCESSOR                                  â”‚
â”‚  â€¢ Generate event ID (UUID)                             â”‚
â”‚  â€¢ Capture timestamp (ISO 8601 UTC)                    â”‚
â”‚  â€¢ Attach actor identity (user ID, email, role)        â”‚
â”‚  â€¢ Collect metadata (project ID, cost, resources)      â”‚
â”‚  â€¢ Generate cryptographic hash (SHA-256)               â”‚
â”‚  â€¢ Sign hash with HSM private key                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚
         â–¼                â–¼
    PostgreSQL        Loki (Log Storage)
    (Persistent)      (Distributed Logs)
         â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFICATION & AUDIT                                   â”‚
â”‚  â€¢ Event signature is immutable (stored with event)    â”‚
â”‚  â€¢ Public key audit: Verify signature matches event    â”‚
â”‚  â€¢ Replay capability: Reconstruct exact execution path â”‚
â”‚  â€¢ Compliance proof: Show regulator "this happened"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consequences

**âœ… Advantages:**
- Immutable proof of what happened (non-repudiation)
- Complies with GDPR Article 5 (integrity & confidentiality)
- Enables full audit trail replay (deterministic reproduction)
- Cryptographic signatures prove nobody tampered with logs
- Loki distributed logs provide full-text searchability
- Events are timestamped UTC (synchronized across regions)

**âš ï¸ Trade-offs:**
- **Cost**: Loki storage + HSM key operations (~â‚¬200/mo additional)
- **Latency**: Signing adds 50-100ms per event
- **Operational Complexity**: Need HSM key rotation procedures
- **Storage**: Audit logs grow ~1GB per 10M events (4 weeks of data)

**ğŸ”§ Mitigation:**
- Use Redis caching layer to batch signatures (reduce latency to 10ms)
- Archive old logs to S3 (cost reduction)
- Automate HSM key rotation (90-day schedule)
- Implement log retention policy (keep 1 year, archive 7 years)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Plain PostgreSQL logs** | Simple, cheap | Easily deleted, no crypto proof | âŒ Rejected |
| **Elasticsearch + checksums** | Searchable, scalable | Checksums can be faked, no signatures | âŒ Rejected |
| **Blockchain (immutable ledger)** | Tamper-proof, cryptographic | Expensive, slow, overkill for this use case | âŒ Rejected |
| **âœ… Event Sourcing + HSM Signatures** | Immutable, searchable, cryptographic proof | Higher cost & complexity | âœ… **CHOSEN** |

### Related ADRs
- ADR-002: Multi-Tenant Isolation (separate logs per tenant)
- ADR-004: OPA Policy Enforcement (policy decisions also signed)
- ADR-005: SLA Monitoring (audit logs feed into SLA calculations)

---

## ğŸ¯ ADR-002: Schema-Per-Tenant Multi-Tenant Data Isolation

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: Database Architect, Security Lead  
**Impact**: HIGH (affects all data architecture)

### Context

FormatDisc serves multiple enterprise clients. We need to ensure:
1. **Tenant A cannot see Tenant B's data** (confidentiality)
2. **Accidental queries don't leak cross-tenant data** (security)
3. **Queries are efficient** (no massive WHERE clauses)
4. **Backups are tenant-specific** (can restore Tenant A without affecting Tenant B)

Single-database (all tenants in one schema with WHERE tenant_id=X) is risky:
- A bug in the query logic leaks data
- Backup/restore is complex (can't restore one tenant independently)

### Decision

Implement **schema-per-tenant** architecture:

```
PostgreSQL Instance (neon.tech)
â”‚
â”œâ”€â”€ public schema (system tables)
â”‚   â”œâ”€â”€ users (master user list)
â”‚   â”œâ”€â”€ tenants (tenant metadata)
â”‚   â”œâ”€â”€ subscriptions (billing)
â”‚   â””â”€â”€ audit_events (cross-tenant audit log)
â”‚
â”œâ”€â”€ tenant_abc123 schema (Tenant ABC's data)
â”‚   â”œâ”€â”€ projects
â”‚   â”œâ”€â”€ agents
â”‚   â”œâ”€â”€ executions
â”‚   â”œâ”€â”€ deployments
â”‚   â””â”€â”€ metrics
â”‚
â”œâ”€â”€ tenant_xyz789 schema (Tenant XYZ's data)
â”‚   â”œâ”€â”€ projects
â”‚   â”œâ”€â”€ agents
â”‚   â”œâ”€â”€ executions
â”‚   â”œâ”€â”€ deployments
â”‚   â””â”€â”€ metrics
â”‚
â””â”€â”€ tenant_def456 schema (Tenant DEF's data)
    â”œâ”€â”€ projects
    â”œâ”€â”€ agents
    â”œâ”€â”€ executions
    â”œâ”€â”€ deployments
    â””â”€â”€ metrics
```

**Implementation Pattern:**

```typescript
// Middleware: Set tenant context for every request
function withTenantContext(handler: Handler) {
  return async (req: Request, context: TenantContext) => {
    // Extract tenant from JWT or API key
    const tenantId = extractTenantId(req);
    
    // Set PostgreSQL search_path to tenant's schema
    await db.query(`SET search_path TO "tenant_${tenantId}",public`);

    try {
      // All queries use tenant schema by default
      return await handler(req, context);
    } finally {
      // Reset to public
      await db.query(`SET search_path TO public`);
    }
  };
}

// Example usage
@withTenantContext
async function getProjects(req: Request, tenantCtx: TenantContext) {
  // This query only sees tenant_abc123.projects
  // Impossible to accidentally see other tenants' data
  const projects = await db.query("SELECT * FROM projects");
  return projects;
}
```

### Consequences

**âœ… Advantages:**
- **Isolation**: Tenant A's data is in a separate schema, unreachable from Tenant B's queries
- **Backup**: Can backup/restore one tenant independently
- **GDPR**: Data deletion is straightforward (drop tenant schema)
- **Compliance**: Auditors understand the isolation guarantee
- **Performance**: No WHERE tenant_id filters needed (less I/O)

**âš ï¸ Trade-offs:**
- **Scaling**: Creating 1000+ schemas can slow down PostgreSQL introspection
- **Migration Complexity**: Schema updates must run on every tenant schema
- **Monitoring**: Need to monitor 1000+ schemas separately
- **Backup Size**: Storing 1000 separate backups is expensive

**ğŸ”§ Mitigation:**
- Use Neon's branching for tenant testing (not separate instances)
- Automate schema migrations (Flyway + Python scripts)
- Use PostgreSQL logical replication for failover (schema-aware)
- Archive old tenant data to S3 (cost reduction)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Row-Level Security (RLS)** | PostgreSQL native, simple | Bugs can leak data, slower than schema isolation | âŒ Rejected |
| **Separate databases per tenant** | Complete isolation | Expensive, hard to manage 1000 databases | âŒ Rejected |
| **Logical databases (Postgres partitioning)** | Scalable, organized | Complex migration, hard to restore single tenant | âŒ Rejected |
| **âœ… Schema-per-tenant** | True isolation, easy backup/restore, good scaling | More schemas to manage | âœ… **CHOSEN** |

### Related ADRs
- ADR-001: Audit Logging (audit table in public schema, cross-tenant)
- ADR-004: OPA Policy Enforcement (tenant context in policies)
- ADR-006: Cost Optimization (per-tenant cost tracking)

---

## ğŸ¯ ADR-003: Blue-Green Zero-Downtime Deployment Strategy

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: DevOps Lead, Release Manager  
**Impact**: HIGH (affects every production release)

### Context

Clients demand 24/7 availability. Any deployment downtime:
- Breaks their MVP simulations (test failures)
- Disrupts production deployments running on our platform
- Violates 99.95% SLA (we can't afford 5 minutes downtime per deployment)

Traditional approach (rolling updates) has downtime window. We need **zero-downtime deployment**.

### Decision

Implement **blue-green deployment** strategy:

```
BEFORE: Blue (current version)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Points to Blue (v1.0)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–¼                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ Blue (v1.0) â”‚  â† ACTIVE         â”‚
â”‚    â”‚ 3 replicas  â”‚                   â”‚
â”‚    â”‚ 100% trafficâ”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ Green (v0.9)â”‚  â† INACTIVE       â”‚
â”‚    â”‚ 0 replicas  â”‚                   â”‚
â”‚    â”‚ 0% traffic  â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DEPLOYMENT PROCESS:
1. Build new version (v1.1, "green")
2. Deploy green alongside blue
3. Run health checks on green
4. Switch load balancer traffic to green (instant)
5. Monitor green for 30 minutes
6. If all good: decommission blue
7. If bad: switch back to blue (instant rollback)

AFTER: Green (new version)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Points to Green (v1.1)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–¼                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ Blue (v1.0) â”‚  â† INACTIVE       â”‚
â”‚    â”‚ 0 replicas  â”‚                   â”‚
â”‚    â”‚ 0% traffic  â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ Green (v1.1)â”‚  â† ACTIVE         â”‚
â”‚    â”‚ 3 replicas  â”‚                   â”‚
â”‚    â”‚ 100% trafficâ”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Kubernetes Implementation:**

```yaml
# Deployment strategy
apiVersion: apps/v1
kind: Deployment
metadata:
  name: formatdisc-app
spec:
  replicas: 3
  strategy:
    type: Blue-Green
    blueGreen:
      activeSlot: blue
      prePromotionHook:
        exec:
          command: ["./health-check.sh"]
      preTerminationHook:
        exec:
          command: ["./graceful-shutdown.sh"]
```

### Consequences

**âœ… Advantages:**
- **Zero downtime**: Instant traffic switch (no in-flight request loss)
- **Instant rollback**: If green has issues, switch back to blue immediately
- **Safe testing**: Can run smoke tests on green before switching
- **A/B testing**: Can gradually shift traffic (green 10% â†’ 50% â†’ 100%)

**âš ï¸ Trade-offs:**
- **Infrastructure cost**: Need 2x resources during deployment (blue + green)
- **Complexity**: Need load balancer with instant switch capability
- **State management**: Stateless services required (no in-memory session state)
- **Database migrations**: Must be backward-compatible (old blue + new green running together)

**ğŸ”§ Mitigation:**
- Use Kubernetes (scales resources on-demand, cost not as bad)
- Session state in Redis (not in-memory)
- Canary deployment (shift 10% traffic to green first, monitor)
- Database migrations in separate job (runs before blue-green switch)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Rolling update** | Simple, gradual | 5-10 min downtime window, complex rollback | âŒ Rejected |
| **Canary deployment** | Gradual, safe | Still has time window of old/new mix | âš ï¸ Partial |
| **âœ… Blue-green** | Zero downtime, instant switch & rollback | 2x cost, more complex | âœ… **CHOSEN** |
| **Red-black (variant of blue-green)** | Same as blue-green | Same as blue-green | âš ï¸ Equivalent |

### Related ADRs
- ADR-005: SLA Monitoring (99.95% uptime depends on blue-green)
- ADR-007: Incident Response (rollback is key incident response)

---

## ğŸ¯ ADR-004: OPA Policy Enforcement for Compliance Gates

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: Compliance Officer, Security Architect  
**Impact**: CRITICAL (affects compliance reporting)

### Context

FormatDisc must verify **every deployment** against compliance rules:
- GDPR: Is /api/user/delete endpoint present?
- SOC2: Is audit logging code present?
- HIPAA: Is encryption implemented?
- Company Policy: No hardcoded secrets, proper error handling

Hard-coding rules in deployment scripts is brittle and hard to audit. We need **declarative, auditable policy engine**.

### Decision

Integrate **OPA (Open Policy Agent)** as compliance gate:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CI/CD Pipeline (GitHub Actions) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage: Build                    â”‚
â”‚  â€¢ Compile, test, build image    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage: Compliance Check (OPA Policy)    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ OPA Evaluates:                   â”‚   â”‚
â”‚  â”‚ â€¢ audit_logging_required()  âœ…   â”‚   â”‚
â”‚  â”‚ â€¢ gdpr_compliant()          âœ…   â”‚   â”‚
â”‚  â”‚ â€¢ zero_hardcoded_secrets()  âœ…   â”‚   â”‚
â”‚  â”‚ â€¢ encryption_required()     âœ…   â”‚   â”‚
â”‚  â”‚ â€¢ sbom_present()            âœ…   â”‚   â”‚
â”‚  â”‚                                  â”‚   â”‚
â”‚  â”‚ Result: PASS or FAIL             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      IF FAIL â”‚ BLOCK
             â”‚
        IF PASS
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage: Deploy to Production     â”‚
â”‚  (only if compliance check PASS) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**OPA Policy Rules (compliance.rego):**

```rego
package formatdisc

# Rule 1: Audit logging is mandatory
audit_logging_required {
    input.code_contains_audit_log_call == true
}

# Rule 2: GDPR requires delete endpoint
gdpr_compliant {
    input.has_delete_user_endpoint == true
}

# Rule 3: No hardcoded secrets (AWS keys, API keys, passwords)
zero_hardcoded_secrets {
    not any_secret_literal_found(input.source_code)
}

# Rule 4: Encryption in transit required
encryption_required {
    input.deployment.tls_version >= "1.3"
    input.deployment.enable_mtls == true
}

# Rule 5: SBOM generated on every build
sbom_present {
    input.build_artifacts.sbom_file_exists == true
}

# DENY if any rule fails
deny[msg] {
    not audit_logging_required
    msg = "BLOCKED: Audit logging code not found. Add 'logAuditEvent()' to critical paths."
}

deny[msg] {
    not gdpr_compliant
    msg = "BLOCKED: GDPR DELETE endpoint required. Add 'POST /api/user/delete' to API."
}

deny[msg] {
    not zero_hardcoded_secrets
    msg = "BLOCKED: Hardcoded secrets detected. Move to environment variables."
}

deny[msg] {
    not encryption_required
    msg = "BLOCKED: TLS 1.3 + mTLS required. Enable in Kubernetes."
}

deny[msg] {
    not sbom_present
    msg = "BLOCKED: SBOM missing. Run 'syft -o json > sbom.json' in CI."
}
```

**Integration in CI/CD:**

```bash
# GitHub Actions workflow
- name: Compliance Check (OPA)
  run: |
    # Prepare input JSON from build artifacts
    opa run -d policy/compliance.rego \
      --input <(cat > input.json <<EOF
{
  "code_contains_audit_log_call": $(grep -q "logAuditEvent" src/ && echo true || echo false),
  "has_delete_user_endpoint": $(grep -q "/api/user/delete" src/routes.ts && echo true || echo false),
  "source_code": "$(cat src/**/*.ts)",
  "deployment": {
    "tls_version": "1.3",
    "enable_mtls": true
  },
  "build_artifacts": {
    "sbom_file_exists": $([ -f sbom.json ] && echo true || echo false)
  }
}
EOF
) \
      --query "data.formatdisc.deny"
    
    # If any deny rules matched, exit non-zero (block deployment)
    if [ $(opa eval ... | jq length) -gt 0 ]; then
      echo "âŒ Compliance check FAILED"
      exit 1
    else
      echo "âœ… Compliance check PASSED"
    fi
```

### Consequences

**âœ… Advantages:**
- **Declarative**: Policies are code (Git-tracked, reviewable, auditable)
- **Enforceable**: Every deployment is validated against policies
- **Auditable**: Policy violations are logged with signatures
- **Flexible**: Can add new policies without code changes
- **Compliant**: Proves to auditors that rules are enforced

**âš ï¸ Trade-offs:**
- **Learning curve**: OPA Rego syntax is unfamiliar to most engineers
- **Maintenance**: Policies must be kept up-to-date as regulations change
- **False positives**: Too strict policies block legitimate deployments
- **Performance**: Policy evaluation adds 30-60 seconds to CI/CD pipeline

**ğŸ”§ Mitigation:**
- Document policies with examples
- Create "policy exceptions" for edge cases (audit each exception)
- Test policies in CI before enforcing
- Use policy library (OpenPolicy/CNCF curated policies)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Hard-coded checks in scripts** | Simple, fast | Not auditable, brittle, duplicate rules | âŒ Rejected |
| **Manual review before deploy** | Thorough | Doesn't scale, human error, slow | âŒ Rejected |
| **âœ… OPA Policy Engine** | Declarative, auditable, enforceable, flexible | Steeper learning curve | âœ… **CHOSEN** |
| **Policy-as-Code (Pulumi)** | Infrastructure-focused | Not designed for compliance rules | âŒ Rejected |

### Related ADRs
- ADR-001: Audit Logging (policies sign their decisions)
- ADR-003: Blue-Green Deployment (policy must pass before blue-green)
- ADR-005: SLA Monitoring (policy compliance affects SLA score)

---

## ğŸ¯ ADR-005: 99.95% SLA Uptime Guarantee with Multi-Region Failover

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: Reliability Engineer, Product Lead  
**Impact**: CRITICAL (affects contract SLA)

### Context

Enterprise customers pay for 99.95% uptime guarantee (22 minutes downtime allowed per year). To deliver this:
1. **Single region insufficient** (can have scheduled maintenance)
2. **Need automatic failover** (no manual intervention)
3. **Need to prove uptime** (auditable metrics)
4. **Need incident response** (minimize MTTR)

### Decision

Implement **multi-region architecture with automatic failover**:

```
ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Global Load Balancer (Cloudflare)                       â”‚
â”‚  â€¢ Route based on latency                                 â”‚
â”‚  â€¢ Health checks every 30 seconds                         â”‚
â”‚  â€¢ Auto-failover if region unhealthy                     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                          â”‚
     â–¼                          â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ US Region   â”‚          â”‚ EU Region    â”‚
  â”‚ (Primary)   â”‚          â”‚ (Secondary)  â”‚
  â”‚             â”‚          â”‚              â”‚
  â”‚ â€¢ K8s 3x    â”‚          â”‚ â€¢ K8s 3x     â”‚
  â”‚ â€¢ PostgreSQLâ”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ PostgreSQL â”‚
  â”‚   (primary) â”‚ wal_level   â”‚ (standby)  â”‚
  â”‚ â€¢ Redis     â”‚ logical_rep â”‚ Redis      â”‚
  â”‚ â€¢ Observ.  â”‚          â”‚ â€¢ Observ.    â”‚
  â”‚ â€¢ 99.99%   â”‚          â”‚ â€¢ 99.99%     â”‚
  â”‚   uptime   â”‚          â”‚   uptime     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ APAC Region (Tertiary)
         â”‚ â€¢ K8s 1x (optional)
         â”‚ â€¢ Read-only replica
         â”‚ â€¢ Async replication
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FAILOVER FLOW:
1. Cloudflare health check to US region fails
2. GLB automatically routes traffic to EU region
3. EU database becomes temporary primary
4. Transactions continue (write-ahead logs)
5. EU replicates to APAC
6. Alert: "US Region Failure" (PagerDuty)
7. On-call investigates US region
8. Once fixed, manual failback (don't auto-switch back)
9. Incident post-mortem (what went wrong?)
```

**SLA Calculation:**

```
Uptime by Component:
  â€¢ US K8s:           99.99% (â‰ˆ50 min/year down)
  â€¢ EU K8s:           99.99% (â‰ˆ50 min/year down)
  â€¢ Global Load Bal:  99.99% (â‰ˆ50 min/year down)

Combined (assuming independence):
  Availability = 1 - (0.0001 Ã— 0.0001 Ã— 0.0001)
               â‰ˆ 100% (highly available!)

In practice (accounting for dependencies):
  Availability â‰ˆ 99.99% Ã— (1 - 0.0001)  [if both regions down]
               â‰ˆ 99.98%

Marketing Promise: 99.95%
(We deliver 99.98%, giving buffer for unexpected issues)

Service Level Indicators (SLIs):
  â€¢ Uptime %: (total_seconds - downtime_seconds) / total_seconds
  â€¢ Error rate: (errors / total_requests) < 0.1%
  â€¢ Latency: p99 < 2 seconds
  â€¢ If any SLI breaches, SLA credit applies

SLA Credit:
  â€¢ 99.95% - 99.0%: 10% refund
  â€¢ 99.0% - 95.0%: 25% refund
  â€¢ < 95%: 100% refund (+ free month)
```

### Consequences

**âœ… Advantages:**
- **High availability**: Automatic failover (no human intervention)
- **Resilience**: One region down doesn't affect customers
- **Scalability**: Distribute load across regions (lower latency globally)
- **Contractual**: Can promise 99.95% SLA (marketable advantage)
- **Disaster recovery**: Data replicated to multiple regions

**âš ï¸ Trade-offs:**
- **Cost**: 2-3x infrastructure cost (pay for multiple regions)
- **Complexity**: Distributed systems are harder to debug
- **Data consistency**: Multi-region replication has lag (eventual consistency)
- **Compliance**: Data residency rules (GDPR: EU data must stay in EU)

**ğŸ”§ Mitigation:**
- Use managed services (Vercel Edge, Supabase multi-region) for cost
- Implement comprehensive monitoring (Prometheus + Grafana)
- Data residency: EU data in EU replicas, US data in US replicas
- Test failover monthly (chaos engineering practice)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **Single region** | Simple, cheap | Can't deliver 99.95%, maintenance downtime | âŒ Rejected |
| **Active-active** | Load balanced, fast failover | Complex, data consistency issues | âš ï¸ Partial |
| **âœ… Active-passive + active-active** | Resilient, high availability, cost-efficient | More infrastructure cost | âœ… **CHOSEN** |
| **Geographic redundancy (multi-cloud)** | Vendor lock-in prevention | Expensive, complex | âŒ Rejected |

### Related ADRs
- ADR-003: Blue-Green Deployment (per-region)
- ADR-002: Multi-Tenant Isolation (tenant data in specific region)
- ADR-007: Incident Response (failover is incident response)

---

## ğŸ¯ ADR-006: FinOps & Cost Optimization Framework

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: Finance, DevOps Lead  
**Impact**: MEDIUM (affects profitability)

### Context

Cloud costs are 13% of revenue (target: <10%). Unchecked infrastructure spending could reduce margins from 78% to <70%.

We need:
1. **Visibility**: Know where every dollar goes
2. **Accountability**: Assign costs to projects/teams
3. **Optimization**: Continuous cost reduction
4. **Budgeting**: Forecast future spend

### Decision

Implement **FinOps (Financial Operations)** framework:

```
COST VISIBILITY:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS CloudWatch + Vercel Dashboard      â”‚
â”‚                                         â”‚
â”‚  Monthly Spend Breakdown:               â”‚
â”‚  â€¢ Kubernetes nodes:        â‚¬2,600 (39%)â”‚
â”‚  â€¢ Database (Neon):         â‚¬800 (12%)   â”‚
â”‚  â€¢ Vercel (hosting):        â‚¬3,200 (48%) â”‚
â”‚  â€¢ Monitoring (DataDog):    â‚¬800 (12%)   â”‚
â”‚  â€¢ Other APIs:              â‚¬200 (-1%)   â”‚
â”‚                                         â”‚
â”‚  Total: â‚¬7,400 per month                â”‚
â”‚  Per project: â‚¬387 (average)            â”‚
â”‚  Gross margin: 78% (78% of revenue)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COST OPTIMIZATION INITIATIVES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Initiative         | Savings  | Status  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reserved Instances  | â‚¬400/mo  | âœ… Done â”‚
â”‚ Spot Instances      | â‚¬250/mo  | âœ… Done â”‚
â”‚ Multi-region balance| â‚¬300/mo  | â³ Plan â”‚
â”‚ DB query optim.     | â‚¬150/mo  | â³ Plan â”‚
â”‚ Image compression   | â‚¬75/mo   | ğŸ“‹ Plan â”‚
â”‚                                         â”‚
â”‚ Total Savings: â‚¬1,175/mo (16%)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COST ALLOCATION BY PROJECT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project         | Cost    | Margin     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Enterprise proj  | â‚¬600    | 92% (high) â”‚
â”‚ Professional     | â‚¬387    | 78% (avg)  â”‚
â”‚ Starter          | â‚¬250    | 65% (low)  â”‚
â”‚                                         â”‚
â”‚ Unprofitable:    | If cost > revenue   â”‚
â”‚  â†’ Stop offering until margin improves  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consequences

**âœ… Advantages:**
- **Visibility**: Know exact cost per project
- **Accountability**: Teams own their cloud spend
- **Optimization**: Can prioritize savings opportunities
- **Profitability**: Maintain 78%+ margins

**âš ï¸ Trade-offs:**
- **Time**: FinOps requires continuous monitoring
- **Complexity**: Need cost allocation algorithms
- **Rigidity**: May limit feature development if too cost-conscious

**ğŸ”§ Mitigation:**
- Automate cost tracking (CloudWatch â†’ Grafana dashboards)
- Monthly cost review (CFO + CTO)
- Set budgets with alerts (prevent overspend)

### Alternatives Considered

| Alternative | Pros | Cons | Verdict |
|------------|------|------|---------|
| **No cost tracking** | Simple | Costs spiral, margins collapse | âŒ Rejected |
| **Quarterly reviews** | Less overhead | Too slow to respond to spikes | âŒ Rejected |
| **âœ… FinOps (weekly reviews)** | Responsive, visible, optimized | Requires discipline | âœ… **CHOSEN** |

### Related ADRs
- ADR-003: Blue-Green Deployment (infrastructure cost)
- ADR-005: SLA Monitoring (cost per SLA tier)

---

## ğŸ¯ ADR-007: Incident Response & Chaos Engineering

**Status**: âœ… Accepted  
**Date**: December 2025  
**Owner**: Reliability Lead, On-Call Manager  
**Impact**: MEDIUM (affects disaster recovery)

### Context

The only way to achieve 99.95% SLA is to be prepared for failures. We need:
1. **Incident response playbook** (what to do when X breaks?)
2. **On-call rotation** (who responds when?)
3. **Chaos engineering** (test failures before they happen in production)
4. **Post-mortems** (learn from incidents)

### Decision

Implement **incident response framework** with chaos tests:

```
INCIDENT SEVERITY LEVELS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SEV-1 (Critical)                        â”‚
â”‚ â€¢ Complete outage (no traffic)          â”‚
â”‚ â€¢ P1: Page on-call immediately          â”‚
â”‚ â€¢ Response: < 5 minutes                 â”‚
â”‚ â€¢ Example: All deployments failing      â”‚
â”‚                                         â”‚
â”‚ SEV-2 (High)                           â”‚
â”‚ â€¢ Degraded service (error rate > 1%)   â”‚
â”‚ â€¢ P1: Page on-call within 15 min       â”‚
â”‚ â€¢ Response: < 15 minutes                â”‚
â”‚ â€¢ Example: One region down              â”‚
â”‚                                         â”‚
â”‚ SEV-3 (Medium)                         â”‚
â”‚ â€¢ Minor issues (error rate 0.1-1%)      â”‚
â”‚ â€¢ P2: Alert on-call, page if continues â”‚
â”‚ â€¢ Response: < 1 hour                    â”‚
â”‚ â€¢ Example: One service intermittently   â”‚
â”‚                                         â”‚
â”‚ SEV-4 (Low)                            â”‚
â”‚ â€¢ Non-urgent (error rate < 0.1%)       â”‚
â”‚ â€¢ Create ticket, no page                â”‚
â”‚ â€¢ Response: < 24 hours                  â”‚
â”‚ â€¢ Example: Typo in error message        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INCIDENT RESPONSE PLAYBOOK:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DETECT (Prometheus/AlertManager)     â”‚
â”‚    Alert fires â†’ PagerDuty page sent    â”‚
â”‚                                         â”‚
â”‚ 2. RESPOND (On-call engineer)           â”‚
â”‚    â€¢ Join Slack #incident channel       â”‚
â”‚    â€¢ Declare SEV level                  â”‚
â”‚    â€¢ Form incident commander + tech     â”‚
â”‚                                         â”‚
â”‚ 3. DIAGNOSE (Debug)                     â”‚
â”‚    â€¢ Check logs (Loki)                  â”‚
â”‚    â€¢ Check metrics (Prometheus)         â”‚
â”‚    â€¢ Check deployments (ArgoCD)         â”‚
â”‚                                         â”‚
â”‚ 4. MITIGATE (Temporary fix)             â”‚
â”‚    â€¢ Rollback if recent deploy          â”‚
â”‚    â€¢ Kill stuck pods (Kubernetes)       â”‚
â”‚    â€¢ Scale up (if load issue)           â”‚
â”‚    â€¢ Manual failover (if region down)   â”‚
â”‚                                         â”‚
â”‚ 5. RESOLVE (Permanent fix)              â”‚
â”‚    â€¢ Fix code (if bug)                  â”‚
â”‚    â€¢ Fix config (if misconfigured)      â”‚
â”‚    â€¢ Deploy fix (blue-green)            â”‚
â”‚                                         â”‚
â”‚ 6. POST-MORTEM (Learn)                  â”‚
â”‚    â€¢ What happened?                     â”‚
â”‚    â€¢ Why did we miss it?                â”‚
â”‚    â€¢ How do we prevent it?              â”‚
â”‚    â€¢ Action items + owner               â”‚
â”‚    â€¢ Public postmortem (transparency)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CHAOS ENGINEERING (Monthly):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Week 1: Kill random pod (chaos monkey)  â”‚
â”‚   â†’ Verify system recovers in < 1 min   â”‚
â”‚                                         â”‚
â”‚ Week 2: Simulate region failure         â”‚
â”‚   â†’ Verify failover works                â”‚
â”‚                                         â”‚
â”‚ Week 3: Simulate database failure       â”‚
â”‚   â†’ Verify backup/restore works          â”‚
â”‚                                         â”‚
â”‚ Week 4: Simulate network latency        â”‚
â”‚   â†’ Verify timeouts work correctly       â”‚
â”‚                                         â”‚
â”‚ Each test generates incident (on purpose)
â”‚ Team practices response procedures       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ON-CALL ROTATION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Monday-Friday: Primary + Secondary      â”‚
â”‚ Saturday-Sunday: One person (reduced)   â”‚
â”‚ Holiday: Pre-arranged coverage          â”‚
â”‚                                         â”‚
â”‚ Handoff: Friday 5pm â†’ Monday 9am        â”‚
â”‚ Sync: 15-min standup (status handoff)   â”‚
â”‚ Comp time: 2 hours comp per night call  â”‚
â”‚ Burnout prevention: Rotate every month  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Consequences

**âœ… Advantages:**
- **Preparedness**: Team knows what to do when incident happens
- **Speed**: Incident response time < 5 min (vs hours without playbook)
- **Learning**: Post-mortems prevent same incident twice
- **Trust**: Customers trust we're prepared for failures

**âš ï¸ Trade-offs:**
- **Time**: On-call duty takes time (comp time helps)
- **Emotional**: Incidents are stressful (blameless culture helps)
- **Overhead**: Chaos tests take time (worth it for reliability)

**ğŸ”§ Mitigation:**
- Blameless post-mortems (focus on systems, not people)
- Adequate comp time (don't burn out on-call engineers)
- Automate chaos tests (reduce manual overhead)

---

## ğŸ“‹ ADR Summary Table

| ADR | Title | Decision | Status | Risk | Impact |
|-----|-------|----------|--------|------|--------|
| **ADR-001** | Immutable Audit Logging | Event sourcing + HSM signatures | âœ… Accepted | Low | CRITICAL |
| **ADR-002** | Multi-Tenant Isolation | Schema-per-tenant | âœ… Accepted | Low | HIGH |
| **ADR-003** | Zero-Downtime Deploy | Blue-green strategy | âœ… Accepted | Low | HIGH |
| **ADR-004** | OPA Policy Enforcement | Compliance gates in CI/CD | âœ… Accepted | Medium | CRITICAL |
| **ADR-005** | 99.95% SLA Uptime | Multi-region active-passive | âœ… Accepted | Medium | CRITICAL |
| **ADR-006** | FinOps Optimization | Weekly cost reviews + budgets | âœ… Accepted | Low | MEDIUM |
| **ADR-007** | Incident Response | Playbooks + chaos engineering | âœ… Accepted | Medium | MEDIUM |

---

## ğŸ”„ Related Decision Dependencies

```
ADR-001 â”€â”€â”€â”€â”€â”€â”
              â”‚
ADR-002 â”€â”€â”¬â”€â”€â”€â”¤
          â”‚   â”‚
ADR-003 â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â†’ ADR-004 (OPA policies)
          â”‚   â”‚       â”‚
ADR-005 â”€â”€â”˜   â”‚       â”‚
              â”‚       â–¼
          ADR-007 (Incident Response)
              â”‚
              â–¼
          ADR-006 (Cost tracking)
```

**Key Dependencies:**
- ADR-001 (Audit) feeds into ADR-004 (Policies) and ADR-007 (Incident Response)
- ADR-003 (Deployment) depends on ADR-004 (Compliance gates)
- ADR-005 (SLA) depends on ADR-003 (Blue-green) and ADR-007 (Failover)

---

**Document Version**: 1.0  
**Last Updated**: December 11, 2025  
**Owner**: Enterprise Architecture Team  
**Status**: âœ… Complete
